{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #matrix math \n",
    "import tensorflow as tf #machine learningt\n",
    "import helpers #for formatting data into batches and generating random sequence data\n",
    "\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0 # padding\n",
    "EOS = 1 # End of sequence\n",
    "\n",
    "vocab_size = 10 # max length of input sequence\n",
    "input_embedding_size = 20 #character length (vector)\n",
    "\n",
    "encoder_hidden_units = 16 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Bidirectional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell =  LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 16) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 16) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 16) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 16) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 16) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 16) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(?, 32) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(?, 32) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "Common part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs as for decoder_lengths\n",
    "max_length = encoder_max_time + 3\n",
    "\n",
    "decoder_inputs = tf.ones([max_length, batch_size], dtype=tf.int32)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "decoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic_rnn decoder\n",
    "For comparison purpose (no attention mechanisms)\n",
    "\n",
    "You can jump direclty to the seq2seq decoder part with running those cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((decoder_outputs),\n",
    " (decoder_state)) = (\n",
    "    tf.nn.dynamic_rnn(cell=decoder_cell,\n",
    "                      inputs=decoder_inputs_embedded,\n",
    "                      sequence_length=decoder_lengths,\n",
    "                      initial_state=encoder_final_state,\n",
    "                      dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=encoder_hidden_units, \n",
    "    memory=attention_states,\n",
    "    memory_sequence_length=[input_embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projection_layer = layers_core.Dense(vocab_size, use_bias=True)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    inputs=decoder_inputs_embedded, \n",
    "    sequence_length=decoder_lengths, \n",
    "    time_major=True)\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    cell=decoder_cell, \n",
    "    helper=helper, \n",
    "    initial_state=encoder_final_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "# Dynamic decoding\n",
    "(decoder_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder,\n",
    "    output_time_major=True\n",
    ")\n",
    "decoder_logits = decoder_outputs.rnn_output\n",
    "decoder_prediction = decoder_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 10) dtype=float32>, sample_id=<tf.Tensor 'decoder/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(?, ?) dtype=int32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[8, 4, 8, 8]\n",
      "[8, 4, 4, 2, 6, 4, 8]\n",
      "[8, 4, 5]\n",
      "[4, 2, 4, 3, 8, 5, 9, 5]\n",
      "[7, 6, 9]\n",
      "[7, 4, 3, 2, 2, 2, 3, 5]\n",
      "[3, 3, 6, 9, 8, 5, 2, 3]\n",
      "[2, 8, 5, 7, 5, 6, 4]\n",
      "[5, 5, 4, 8, 4, 2, 9, 4]\n",
      "[3, 3, 3, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.15325832367\n",
      "  sample 1:\n",
      "    input     > [2 9 7 8 0 0 0 0]\n",
      "    predicted > [3 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 2 8 6 6 3 9 5]\n",
      "    predicted > [0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 6 7 3 3 2 9 7]\n",
      "    predicted > [9 9 9 9 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.513044953346\n",
      "  sample 1:\n",
      "    input     > [4 5 8 7 9 0 0 0]\n",
      "    predicted > [4 5 7 9 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 8 0 0 0 0 0]\n",
      "    predicted > [8 8 8 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 5 2 5 8 3 3 0]\n",
      "    predicted > [5 3 3 3 3 3 3 1 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.28789794445\n",
      "  sample 1:\n",
      "    input     > [9 7 4 3 0 0 0 0]\n",
      "    predicted > [9 7 4 3 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 2 5 5 0 0 0 0]\n",
      "    predicted > [3 2 5 5 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 5 9 6 4 7 8]\n",
      "    predicted > [8 9 4 4 4 8 7 8 1 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.169585168362\n",
      "  sample 1:\n",
      "    input     > [5 2 7 9 0 0 0 0]\n",
      "    predicted > [5 2 7 9 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 9 7 3 7 6 6 3]\n",
      "    predicted > [2 9 7 6 6 6 6 3 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 9 6 3 4 0 0 0]\n",
      "    predicted > [4 9 6 3 4 1 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1710 after 384128 examples (batch_size=128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgz6REQh8v9vcNlBJgcbLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128fdb990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
