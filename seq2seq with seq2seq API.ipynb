{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import helpers #for formatting data into batches and generating random sequence data\n",
    "\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0 # padding\n",
    "EOS = 1 # End of sequence\n",
    "\n",
    "vocab_size = 10 # max length of input sequence\n",
    "input_embedding_size = 20 #character length (vector)\n",
    "\n",
    "encoder_hidden_units = 16 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "Bidirectional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell =  LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 16) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 16) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 16) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 16) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 16) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 16) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenates tensors along one dimension.\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(?, 32) dtype=float32>, h=<tf.Tensor 'concat_2:0' shape=(?, 32) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "Common part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs as for decoder_lengths\n",
    "max_length = encoder_max_time + 3\n",
    "\n",
    "decoder_inputs = tf.ones([max_length, batch_size], dtype=tf.int32)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "decoder_inputs_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic_rnn decoder\n",
    "For comparison purpose (no attention mechanisms)\n",
    "\n",
    "You can jump direclty to the seq2seq decoder part with running those cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((decoder_outputs),\n",
    " (decoder_state)) = (\n",
    "    tf.nn.dynamic_rnn(cell=decoder_cell,\n",
    "                      inputs=decoder_inputs_embedded,\n",
    "                      sequence_length=decoder_lengths,\n",
    "                      initial_state=encoder_final_state,\n",
    "                      dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
    "#weights\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to convert output to human readable prediction\n",
    "#we will reshape output tensor\n",
    "\n",
    "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "#reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=decoder_hidden_units, \n",
    "    memory=attention_states,\n",
    "    #memory_sequence_length=None # default value\n",
    ")\n",
    "\n",
    "decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    cell=decoder_cell, \n",
    "    attention_mechanism=attention_mechanism,\n",
    "    attention_layer_size=decoder_hidden_units)\n",
    "\n",
    "attention_zero = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "encoder_final_state = attention_zero.clone(cell_state=encoder_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projection_layer = layers_core.Dense(vocab_size, use_bias=True)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    inputs=decoder_inputs_embedded, \n",
    "    sequence_length=decoder_lengths, \n",
    "    time_major=True)\n",
    "\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    cell=decoder_cell, \n",
    "    helper=helper, \n",
    "    initial_state=encoder_final_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "# Dynamic decoding\n",
    "(decoder_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder,\n",
    "    output_time_major=True\n",
    ")\n",
    "decoder_logits = decoder_outputs.rnn_output\n",
    "decoder_prediction = decoder_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 10) dtype=float32>, sample_id=<tf.Tensor 'decoder/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(?, ?) dtype=int32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "#one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[9, 6, 8, 6]\n",
      "[3, 7, 3]\n",
      "[5, 6, 7, 8, 4, 7, 8]\n",
      "[3, 3, 2, 3, 4, 4, 9]\n",
      "[7, 3, 4, 3, 8, 3]\n",
      "[7, 5, 7]\n",
      "[2, 5, 2, 7, 2, 6, 7]\n",
      "[4, 9, 9, 6, 2, 8]\n",
      "[2, 6, 8, 4, 7, 4, 8]\n",
      "[8, 5, 3, 4, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.29568171501\n",
      "  sample 1:\n",
      "    input     > [4 5 7 0 0 0 0 0]\n",
      "    predicted > [6 5 4 4 4 4 4 4 4 4 4]\n",
      "  sample 2:\n",
      "    input     > [2 6 9 6 8 6 3 2]\n",
      "    predicted > [9 5 5 5 5 5 5 5 5 5 5]\n",
      "  sample 3:\n",
      "    input     > [2 4 8 6 8 9 2 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 5 5]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.00324463192374\n",
      "  sample 1:\n",
      "    input     > [9 3 8 7 3 5 7 8]\n",
      "    predicted > [9 3 8 7 3 5 7 8 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 2 9 3 2 0 0 0]\n",
      "    predicted > [2 2 9 3 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 7 7 0 0 0 0 0]\n",
      "    predicted > [3 7 7 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.000481951137772\n",
      "  sample 1:\n",
      "    input     > [7 4 5 6 9 7 5 0]\n",
      "    predicted > [7 4 5 6 9 7 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 2 0 0 0 0 0]\n",
      "    predicted > [5 5 2 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 5 2 4 4 4 2 2]\n",
      "    predicted > [4 5 2 4 4 4 2 2 1 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.000864942325279\n",
      "  sample 1:\n",
      "    input     > [4 5 3 6 8 0 0 0]\n",
      "    predicted > [4 5 3 6 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 4 6 8 2 5 0]\n",
      "    predicted > [4 8 4 6 8 2 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 2 5 0 0 0 0 0]\n",
      "    predicted > [7 2 5 1 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't forget that the '1' at the end of the sequences is the EOS tag, not a predicted output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0009 after 384128 examples (batch_size=128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niTj0zcxK5P8DpNAgaO/UA9kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a3d19d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
